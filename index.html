<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Final Project: Music to My Ears</title>
  <style>
    body {
      font-family: 'Comic Sans MS', cursive, sans-serif;
      margin: 0;
      padding: 0;
    }
    .container {
      max-width: 900px;
      margin: auto;
      padding: 20px;
      text-align: justify;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
    }
    table, th, td {
      border: 1px solid black;
    }
    th, td {
      padding: 10px;
      text-align: left;
    }
    h1, h2, h3 {
      text-align: center;
    }
    a {
      color: blue;
    }
    .center-media {
      display: flex;
      justify-content: center;
      margin: 20px 0;
    }
    .photo-album {
      display: flex;
      gap: 20px;
      flex-wrap: wrap;
      justify-content: center;
    }
  </style>
</head>
<body>

<div class="container">

<h1>Final project: Music to my Ears</h1>
<h3>Melina Daniilidis, Garrett Kirsch, Nikolaos Rapanis</h3>

<h2>Results</h2>
<p>Our final device, Music to My Ears, successfully reads live MIDI input from an electric keyboard, allows the user to shift the melody up or down by a desired number of semitones using two buttons, and replays the transposed melody through a buzzer with accurate timing. The system also displays the inputted and shifted notes on a color LCD screen for easy visual feedback. It uses an ATmega328PB microcontroller to manage real-time MIDI UART communication, PWM audio output, and user input simultaneously. We implemented a full graphics library for musical notation display, wrote custom drivers for the buzzer and screen, and built a simple cardboard housing to organize the components neatly on the workbench. In the end, we delivered a working, responsive prototype that achieves the original project goal: enabling musicians to quickly and easily transpose melodies without needing music theory knowledge.</p>

<h2>Video Demonstration</h2>
<div class="center-media">
  <iframe src="https://drive.google.com/file/d/1ngGfAtsrGlymLhpcTdSK29lmTKguykDP/preview" width="640" height="480" allow="autoplay"></iframe>
</div>

<h2>Photo album</h2>
<div class="photo-album">
  <img src="Images/IMG_3281.jpg" alt="Device Photo 1" width="300">
  <img src="Images/IMG_3282.jpg" alt="Device Photo 2" width="300">
  <img src="Images/IMG_3315.PNG" alt="Device Photo 3" width="300">
</div>

<h2>Software Requirements Specification Validation</h2>

<table>
  <tr>
    <th>ID</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>SRS-01</td>
    <td>The notes from the input recording shall be cleaned and parsed accurately based on their frequencies.</td>
  </tr>
  <tr>
    <td>SRS-02</td>
    <td>The screen will show the number of times the shift button has been pressed, so the user has a visual indication of how many semitones they want to shift by.</td>
  </tr>
  <tr>
    <td>SRS-03</td>
    <td>The transposition algorithm will correctly shift each note of the inputted melody by the user's desired amount of semitones.</td>
  </tr>
  <tr>
    <td>SRS-04</td>
    <td>The screen should correctly display the names of the notes from the transposed melody in a readable format, so the user can try to play the transposed melody on the keyboard if they so choose.</td>
  </tr>
  <tr>
    <td>SRS-05</td>
    <td>The speaker shall correctly output the transposition melody. We will test on the same types of inputs as the transposition algorithm and in similar order.</td>
  </tr>
  <tr>
    <td>SRS-06</td>
    <td>The screen should display the transposed notes and the speaker will play the frequencies of the transposed notes simultaneously.</td>
  </tr>
</table>

<p><strong>What worked well:</strong><br>
We were able to read MIDI input reliably, store melodies, shift notes by a user-defined semitone amount, and display both the original and shifted notes on the LCD in real-time.</p>

<p><strong>What changed:</strong><br>
Initially, we planned for selecting input and output keys (e.g., C major to A minor). Midway through development, we changed to simple up/down relative semitone shifting to avoid needing user key input and to simplify button controls. This also removed the need to validate user-selected keys against the melody.</p>

<p><strong>New Requirements Added:</strong><br>
We added requirements for button-controlled shifting and displaying the shift count on the screen.</p>

<p><strong>Challenges:</strong><br>
Handling interrupts while using UART-based MIDI input was challenging so we had to resort to using polling for the buttons, until we eventually managed to use interrupts without issues just before the final demonstration.</p>

<p><strong>Validation</strong><br>
SRS-01: We have validated the input reading by printing the MIDI note to the serial terminal whenever it is played as demonstrated on the video below:</p>
<div class="center-media">
  <iframe 
    src="https://drive.google.com/file/d/1kMERMMm2DjY6He5HBS92R28iUckF7n3f/preview" 
    width="360" 
    height="640" 
    style="border: none;" 
    allow="autoplay">
  </iframe>
</div>
<p>SRS-06: We have met this requirement, as evident in the demo video. Specifically, during the listening mode the LCD screen displays the current played note and the buzzer outputs the corresponding sound.</p>

<h2>Hardware Requirements Specification Validation</h2>

<table>
  <tr>
    <th>ID</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>HRS-01</td>
    <td>A mini electric piano will be used to provide inputs (i.e. music) that will then be processed by the atmega.</td>
  </tr>
  <tr>
    <td>HRS-02</td>
    <td>There will be two buttons to choose the shift amount (either to a higher or lower note) by a semitone for each button press.</td>
  </tr>
  <tr>
    <td>HRS-03</td>
    <td>A screen will be used to display the input key, output key, input octave, and output octave of the music.</td>
  </tr>
</table>

<p><strong>What worked well:</strong><br>
The MIDI communication circuit using UART was stable after bypassing the original optoisolator. Our LCD screen and buzzer drivers functioned reliably. Button presses for shifting semitones worked correctly with interrupts, avoiding UART conflicts.</p>

<p><strong>What changed:</strong><br>
Initially, we expected the piano to have a built-in speaker, but it did not. We added a separate buzzer system instead. Additionally, we planned to have separate speakers for listening and playback mode, however, we decided that this would be redundant.</p>

<p><strong>Challenges:</strong><br>
The circuitry proposed by the official MIDI protocol documentation did not provide an output (potentially due to a faulty opto-isolator). Due to the simple nature of our circuit, we decided not to use it at all with the potential risk of not getting all the signals (something that thankfully never happened).</p>

<p><strong>Validation</strong><br>
HRS-01: We have met this requirement alongside SRS-01. The video showing MIDI input captured live with UART printout confirmation.<br>
HRS-02: The requirement is validated by the final demonstration video where we press the buttons, changing the display and getting back a different sound in playback mode.</p>

<h2>Reflection</h2>

<p><strong>What did we learn?</strong><br>
We built a complete embedded system that integrates UART-based MIDI communication, real-time audio output via PWM, and graphical output through an LCD, all controlled by user input. Also, we learned how to adapt a project based on our hardware limitations and integration challenges.</p>

<p><strong>What went well:</strong><br>
Our LCD graphics library, UART MIDI parsing, and note-shifting logic all worked together smoothly. The display of notes and the buzzer's frequency output worked reliably once we had the right hardware in place. Our decision to simplify the transposition interface to semitone shifting also made the user experience much more intuitive.</p>

<p><strong>What accomplishments are we proud of:</strong><br>
We are especially proud that our device can receive live MIDI input from a real electric piano, shift melodies in real-time, and provide visual and audio feedback using custom-written drivers without any delays. Despite setbacks with and design changes, we produced a functioning musical tool that achieves our original goal.</p>

<p><strong>What did we gain from this experience?</strong><br>
Hands-on experience with the challenges of integrating multiple subsystems on a resource-constrained microcontroller, as well as the discipline required to adjust our design without compromising functionality. We also learned the value of thorough debugging, especially when dealing with asynchronous serial protocols and timing conflicts.</p>

<p><strong>Did we have to change our approach?</strong><br>
Yes. Originally, we planned to allow the user to transpose a melody from one musical key to another (e.g., C major to G minor). However, this approach required additional user input and key signature validation, which added unnecessary complexity. We switched to a semitone-based shifting method using two buttons (up/down), which was simpler to implement, more user-friendly, and more robust for real-time performance.</p>

<p><strong>What could have been done differently?</strong><br>
We could have built our own mock keyboard using switches and a resistive network with an ADC. This would have removed our dependency on external parts (the MIDI keyboard), which arrived later than expected and slowed early progress. Designing our own input hardware could have also given us more control over note timing and synchronization.</p>

<p><strong>Did we encounter unexpected obstacles?</strong><br>
Yes â€” the biggest was our initial use of the opto-isolator for MIDI input, following the official MIDI circuit documentation. Despite proper setup, it failed to produce any readable output, likely due to a faulty opto-isolator. After debugging, we decided to remove the opto-isolator entirely and directly receive the MIDI signal. While this technically violated the MIDI electrical spec, it worked reliably for our simple circuit without any observed signal loss.</p>

<p><strong>What could be a next step for this project?</strong><br>
The most natural next step would be supporting polyphonic input, i.e., handling chords. Right now, our system assumes one note at a time. Expanding it to handle simultaneous notes would require reworking our melody data structure and output timing, but it would make the tool far more useful to musicians working with real music.</p>

</div>

</body>
</html>
